\section{问题提出的背景}

% \par 正文格式与具体要求\cite{zjuthesisrules}

\subsection{背景介绍}
\subsubsection{大语言模型对齐的动机与挑战}
随着深度学习和预训练范式的不断演进，大语言模型（LLM）已经成为人工智能领域中最引人注目的技术之一。
诸如 GPT 系列、PaLM、LLaMA 等大模型在多种自然语言处理任务上取得了超越传统方法的成果。
然而，伴随模型规模日益扩大，\textbf{对齐（alignment）}问题也愈发凸显。
所谓“对齐”，即确保模型的行为与人类的价值观、道德观和期望保持一致，从而在减少错误或有害输出的同时，满足特定应用场景的需求。
在对齐训练中最典型的思路，是通过\textbf{从人类反馈中强化学习}（RLHF, Reinforcement Learning from Human Feedback）来微调大模型，
使其在回答问题时更贴近人类偏好。不过，这种方法依赖大量人工标注的偏好数据，不仅成本高昂，也难以在更大规模与更广泛领域上应用。
与此同时，“人类偏好”本身也带有主观性，并非适用于所有任务场景；在某些有客观标准的领域，例如数学推理或编程任务，人工标注的效用并不一定最优.

为此，业界和学术界越来越关注\textbf{基于规则反馈信号（Rule-based Feedback）}的方法，
以减少对人类监督的依赖或与人类反馈结合，从而更高效、更广泛地对齐大模型的行为。
所谓“规则反馈信号”，可由人工编写或模型自动生成的一系列准则、程序或约束，用来直接给模型的输出打分、施加惩罚或奖励。
根据这些规则的形式与自动化程度，相关方法呈现出从\textbf{显式规则}到\textbf{隐式自监督}的丰富谱系。
依赖客观标准的显式规则既能节省大笔人力标注，也能更精准地保证某些关键维度上的正确性；
而偏向隐式方法的自监督方式，则在主观问题或大规模数据生成中具有强大的伸缩性与自动化优势.
与此同时，行业与学术的最新突破还将\textbf{博弈论、最优控制、约束优化}等数学工具引入对齐训练中
为“基于规则反馈信号”这一领域提供了新的理论支撑与方法创新。

大预言模型对齐人类偏好的训练面临着大量挑战。最核心的困难在于以下几点。
\textbf{大规模模型的复杂性：}
现代大模型通常有数十亿到数千亿乃至万亿级参数，其内部表示和决策过程复杂且不透明，
这种黑箱系统中很难施加并维持一组对齐约束，既要保证核心任务性能，又要避免产生与人类价值观相背的输出；
\textbf{多样性与主观性：}
大模型面临的任务和应用极度多样；有些问题存在客观唯一的正确答案（如编程、数学），另一些问题则充满主观性（如创意写作、社会价值），
单一的人工偏好或简单的规则难以覆盖所有情形，导致对齐方法需要更具灵活性和适应性；
\textbf{人力成本与可扩展性：}
RLHF 等方法依赖数万甚至数十万条人工打分或标注数据，随着模型规模和应用领域扩张，纯粹依赖人工难以为继，迫切需要高效率、低人力消耗的对齐途径。

基于以上动机和挑战，研究者们开始尝试\textbf{“基于规则反馈信号”}的各种方法，
希望借助显式、可编程的规则和隐式自监督的扩展技巧，实现更高效、更有针对性的对齐. 

\subsubsection{基于规则的反馈方法}
基于规则反馈的主流方法包括\textbf{显式规则}、\textbf{规则+模型裁决} 和 \textbf{隐式自监督信号}几种主要方法。

\textbf{显式规则 }指的是由人类手动编写或直接用程序实现的明确准则，用于判断模型输出的好坏、对错或合规/违规情况，
常应用于数学推理（比对标准答案），编程生成（通过测试用例判断是否正确），事实问答（与标准知识库对照），以及安全过滤（关键词屏蔽、禁用政策等）。
显式规则精确可控且无需额外设计奖励模型，但对于主观性较强的任务无法穷尽所有输出而难以胜任。

\textbf{规则+模型裁决 }往往在任务目标既包含客观要素，也涉及主观价值或复杂上下文时使用。
在此类方法中，规则通常提供最低限度或关键维度的“硬性约束”，
而模型自身（或额外训练的判别器）则在更主观、更模糊的维度上给出辅助评估，实现了一种“半自动”且相对可扩展的对齐流程。
例如 AI 法官\citep{bai2022constitutional}、DeepMind Sparrow 等。
半自动的规则有一定自动化程度，人工参与相对减少，
但仍需人类设定或审校核心规则，且对规则理解偏差可能影响模型行为。

\textbf{隐式自监督信号 }则力图尽量脱离人工编写的具体规则或广泛的人类偏好数据，转而让模型自行生成或推断对“更优”输出的偏好。
例如 SPA\citep{Kim2025Spread}，LLM-as-Judge\citep{Li2024LLMs}，RLAIF\citep{Lee2024Rlaif} 等。
这种模型大幅降低人力成本，可在无须大量人工标注的情况下迅速扩充偏好数据；
但容易出现“自我偏见”、偏差放大、价值观漂移等风险，需要额外校准与审查。

在不同场景下，规则反馈方法呈现出一条从显式规则到隐式自监督的连续“频谱”。
基于模型隐式自监督信号是从显式规则的良好延伸，包含了对合理性、正确性、安全性等既定规则等先验判断。

\subsubsection{从归纳-演绎的角度看待基于规则的反馈方法}
\textbf{归纳}是从具体的观测、实例或经验事实出发，通过总结、概括，得出更一般化或抽象化的规律、模型或假设。
它通过观察若干具体例子总结共同特征或规律，从而提出较为通用的“规则”。
然而，归纳得到的规律并不保证绝对正确，只是“相对可信”或“在目前观察的范围内大体成立”。今后若发现反例，需要修正或推翻。
\textbf{演绎}则是从已有的公理、规则或理论出发，将其应用到具体场景或个案中去推导结论。
它基于已有的普适法则推断出具体情况下的结论，若逻辑推理严谨且前提有效，所得结论必然正确。
演绎依赖前提的正确性和完备性，若前提（公理或规则）本身有缺陷，则推理结论同样会受到影响。
在新知识的产生过程中，归纳与演绎往往是相辅相成的：
我们先通过归纳（从数据、现象中得出一个“可能的”规则），
再在后续实践或应用（演绎）中验证、完善或修正该规则，
这样便推动了对现有知识的扩展或对新知识的发现。

在“大语言模型对齐”或“机器学习模型训练”中，
“基于规则的反馈”指的是预先由人类或程序所制定（或学得）的一组\textbf{规则}，用来对模型输出进行评估、打分、惩罚或奖励。
此类规则有时是显式的（如编程测评的单元测试，安全过滤的关键词屏蔽），
有时是半显式或隐式的（如由强模型、专家系统或复杂程序给出判定分值）。
这些规则通常包含了对正确性、合规性或安全性的先验判断。
无论是人工编写还是通过模型提炼，一开始往往会基于大量实例或经验做出总结——这就是\textbf{归纳}的过程。
例如，在数学/编程评测中，人们归纳出若干“正确答案的判定标准”或“需要通过的测试用例”。
在对话合规性中，安全准则也往往来自过去对违规内容的收集、对社会规范的总结等。
这些归纳出来的规则通常具有“一般性”或“抽象度”，使得它们能够适用到更广泛的情境中，而不仅仅局限于最初观测到的少数实例。
一旦这些规则被定义或学得，系统（或模型）就能够利用这些规则对新输出进行评估，这就是\textbf{演绎}。
从“通用规则”出发，针对具体问题或输出进行判断：若满足规则则给予奖励，不满足则惩罚。
在运行时，对模型一次次的输出进行演绎式的“匹配或推理”：判定此刻的输出是否合规、正确或符合此前总结出来的规则。
当反馈信号与模型的输出不匹配时，系统会在后续训练中进行\textbf{修正}（或人类会重新审视规则）。
如果在演绎应用的过程中发现了足够多的反例，那么对原先规则的修订又会再次调用“归纳”环节的思维过程：根据新的实例或异常情况，完善或替换旧规则。
在实际大模型的对齐或对抗训练中，模型的“错误”或“违规”输出起到“反例”的作用，
让我们不断迭代规则或权重，从而使系统获得更高的精确度和泛化能力，这也是新知识（或更好的规则）产生和积累的过程。

\subsection{本研究的意义和目的}

随着大语言模型在各个领域的广泛应用，其对齐度和任务适应性成为了关键挑战。传统的对齐方法通常依赖大量人工偏好数据，
这在许多实际应用中难以实现。现有方法往往面临数据稀缺和标注成本高的问题，尤其是当面对噪声数据时，
模型的对齐效果可能会大打折扣。因此，如何在少量人工标注数据的基础上，提高模型对人类偏好的对齐效率，
并确保对注释噪声具有足够的鲁棒性，成为了亟待解决的问题。

为了应对这一挑战，我希望提出一种基于对抗性训练的偏好优化算法，这是一种采用基于模型隐式规则的反馈进行自增强训练的方法，
其中的对抗训练部分，结合了归纳-演绎法的思路，旨在构造具有挑战性的实例，激活模型不断的归纳新的知识，
以适应人类偏好规则。
本研究希望结合数据扩展与自我优化策略，旨在通过多轮对抗性训练，
逐步优化大语言模型的对齐效果。通过生成对抗样本并进行自我标注，该方法能够减少人工偏好数据的需求，
同时提升模型在面对噪声数据时的鲁棒性。此外，在对抗性训练的过程中，模型能够调整自身的偏好判断能力，
确保在任务适应性上表现出更高的稳定性与可靠性。

这一方法的最大意义在于，它不仅可以减少对大量人工标注数据的依赖，还能在噪声环境中有效保持模型的对齐精度，
尤其在数据稀缺或标注困难的场景中，具有广泛的应用前景。通过引入对抗性训练机制，能够提升模型的自我纠正能力，
增强其在实际任务中的适应性，确保偏好数据的高效利用与模型对齐的持续优化。通过这种方式，能够在保证模型鲁棒性的同时，
实现更高效的数据利用和训练过程，从而为大语言模型的实际应用提供一种更加可行且经济的解决方案。